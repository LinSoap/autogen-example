from typing import (
    Annotated,
    Any,
    AsyncGenerator,
    Awaitable,
    Callable,
    List,
    Literal,
    Optional,
    Sequence,
)

from autogen_agentchat.ui import Console
from autogen_agentchat.base import TaskResult
from autogen_agentchat.agents import BaseChatAgent, AssistantAgent
from autogen_agentchat.base import Response
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.tools import TeamTool
from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType
from autogen_agentchat.messages import (
    BaseAgentEvent,
    BaseChatMessage,
    TextMessage,
    StructuredMessage,
    MemoryQueryEvent,
)
from autogen_agentchat.conditions import SourceMatchTermination
from autogen_core import CancellationToken
from autogen_core.memory import Memory
from autogen_core.tools import BaseTool, FunctionTool
from autogen_core.models import CreateResult
from autogen_core.model_context import (
    ChatCompletionContext,
    UnboundedChatCompletionContext,
)
from config.model_config import model_client
from pydantic import BaseModel
import asyncio

# -------------------- ç±»å‹å’Œæ•°æ®æ¨¡å‹ --------------------


class TaskRunnerToolArgs(BaseModel):
    """Input for the TaskRunnerTool."""

    task: Annotated[str, "The task to be executed."]


class WordInsightAnalysis(BaseModel):
    """insight_agent çš„ç»“æ„åŒ–åˆ†ææ¨¡å‹"""

    class ExistingInformation(BaseModel):
        """å·²æœ‰ä¿¡æ¯ç¡®è®¤"""

        document_type: str  # æ–‡æ¡£ç±»å‹
        target_audience: str  # ç›®æ ‡å—ä¼—
        writing_purpose: str  # å†™ä½œç›®çš„
        style_requirement: str  # é£æ ¼è¦æ±‚
        key_content: List[str]  # ç”¨æˆ·æä¾›çš„å…³é”®å†…å®¹è¦ç‚¹

    class SupplementaryQuestion(BaseModel):
        """è¡¥å……ä¿¡æ¯é—®é¢˜"""

        question: str  # æå‡ºçš„å…·ä½“é—®é¢˜
        options: List[str]  # é—®é¢˜é€‰é¡¹ï¼šä¾›ç”¨æˆ·é€‰æ‹©æˆ–å‚è€ƒï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå¼€æ”¾å¼é—®é¢˜
        reason: str  # éœ€è¦æ­¤ä¿¡æ¯çš„åŸå› 
        type: Literal[
            "open", "single_choice", "multiple_choice"
        ]  # "open" | "single_choice" | "multiple_choice"ï¼Œä¾›å‰ç«¯è¯†åˆ«æ¸²æŸ“

    existing_information: ExistingInformation
    supplementary_questions: List[SupplementaryQuestion]


# -------------------- å·¥å…·å’Œ Agent ç±»å®šä¹‰ --------------------


class ToolCallAgent(BaseChatAgent):
    def __init__(
        self,
        name: str,
        description: str = "An agent that provides assistance with ability to use tools.",
        model_context: ChatCompletionContext | None = None,
        memory: Sequence[Memory] | None = None,
        tools: (
            List[
                BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]
            ]
            | None
        ) = None,
    ):
        super().__init__(name=name, description=description)
        self._memory = None
        if memory is not None:
            if isinstance(memory, list):
                self._memory = memory
            else:
                raise TypeError(
                    f"Expected Memory, List[Memory], or None, got {type(memory)}"
                )
        self._tools: List[BaseTool[Any, Any]] = []
        if tools is not None:
            for tool in tools:
                if isinstance(tool, BaseTool):
                    self._tools.append(tool)
                elif callable(tool):
                    if hasattr(tool, "__doc__") and tool.__doc__ is not None:
                        description = tool.__doc__
                    else:
                        description = ""
                    self._tools.append(FunctionTool(tool, description=description))
                else:
                    raise ValueError(f"Unsupported tool type: {type(tool)}")
        # ç¡®ä¿å·¥å…·åç§°å”¯ä¸€
        tool_names = [tool.name for tool in self._tools]
        if len(tool_names) != len(set(tool_names)):
            raise ValueError(f"Tool names must be unique: {tool_names}")

        if model_context is not None:
            self._model_context = model_context
        else:
            self._model_context = UnboundedChatCompletionContext()

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                return message
        raise AssertionError("The stream should have returned the final result.")

    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        agent_name = self.name
        memory = self._memory
        model_context = self._model_context

        # STEP 1: Add new user/handoff messages to the model context
        await self._add_messages_to_context(
            model_context=model_context,
            messages=messages,
        )

        # STEP 2: Update model context with any relevant memory
        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []
        for event_msg in await self._update_model_context_with_memory(
            memory=memory,
            model_context=model_context,
            agent_name=agent_name,
        ):
            inner_messages.append(event_msg)
            yield event_msg

        # STEP 3: Generate a message ID for correlation between streaming chunks and final message

        async for inference_output in self._tools[0].run_stream(
            args=TaskRunnerToolArgs(task=messages[-1].content),
            cancellation_token=cancellation_token,
        ):
            if isinstance(inference_output, TaskResult):
                await model_context.add_message(
                    inference_output.messages[-1].to_model_message()
                )
                yield Response(chat_message=inference_output.messages[-1])
            else:
                # Streaming chunk event
                yield inference_output

    @staticmethod
    async def _add_messages_to_context(
        model_context: ChatCompletionContext,
        messages: Sequence[BaseChatMessage],
    ) -> None:
        """
        Add incoming messages to the model context.
        """
        for msg in messages:
            await model_context.add_message(msg.to_model_message())

    @staticmethod
    async def _update_model_context_with_memory(
        memory: Optional[Sequence[Memory]],
        model_context: ChatCompletionContext,
        agent_name: str,
    ) -> List[MemoryQueryEvent]:
        """Update model context with memory content.

        Args:
            memory: Optional sequence of memory stores to query
            model_context: Context to update with memory content
            agent_name: Name of the agent for event tracking

        Returns:
            List of memory query events generated during update
        """
        events: List[MemoryQueryEvent] = []
        if memory:
            for mem in memory:
                update_context_result = await mem.update_context(model_context)
                if (
                    update_context_result
                    and len(update_context_result.memories.results) > 0
                ):
                    memory_query_event_msg = MemoryQueryEvent(
                        content=update_context_result.memories.results,
                        source=agent_name,
                    )
                    events.append(memory_query_event_msg)
        return events

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass


async def run_tool_call_agent() -> None:
    user_memory = ListMemory()

    # Add user preferences to memory
    await user_memory.add(
        MemoryContent(content="æˆ‘æ˜¯ä¸€ä¸ªåº”å±Šæ¯•ä¸šç”Ÿ", mime_type=MemoryMimeType.TEXT)
    )
    await user_memory.add(
        MemoryContent(content="æˆ‘ä»Šå¹´22å²", mime_type=MemoryMimeType.TEXT)
    )
    await user_memory.add(
        MemoryContent(content="æˆ‘æ˜¯ç”·ç”Ÿ", mime_type=MemoryMimeType.TEXT)
    )
    await user_memory.add(
        MemoryContent(
            content="2025å¹´7æœˆ12æ—¥ä¸ºæ­¢ï¼Œæˆ‘å·²ç»åœ¨éº¦å½“åŠ³å®ä¹ ç¬¬å…­å‘¨",
            mime_type=MemoryMimeType.TEXT,
        )
    )
    await user_memory.add(
        MemoryContent(content="æˆ‘ä¸»è¦åœ¨åå¨ç‚¸è–¯æ¡", mime_type=MemoryMimeType.TEXT)
    )
    # -------------------- Agent/Team/Tool å®ä¾‹åŒ–ä¸ä¸»æµç¨‹ --------------------

    json_agent = AssistantAgent(
        name="json_agent",
        description="è¿™æ˜¯ä¸€ä¸ªJSONç»“æœæ£€éªŒå·¥å…·ï¼Œä¼šæ ¹æ®è¾“å…¥è¾“å‡ºå¹²å‡€çš„JSONç»“æœ",
        model_client=model_client,
        output_content_type=WordInsightAnalysis,
        system_message="""
        ä½ å¯¹è¾“å…¥çš„JSONæ•°æ®åšæ£€éªŒï¼Œè§£å†³å¸¸è§çš„JSONæ ¼å¼é”™è¯¯å¹¶è¾“å‡ºå¹²å‡€çš„JSONç»“æœ,è¿™ä¸ªæ˜¯å¯¹åº”çš„Pydanicæ¨¡å‹
        ä½ åªè´Ÿè´£è§£å†³JSONæ ¼å¼é”™è¯¯ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆä»¥ä¸‹ç»“æ„åŒ–æ¨¡å‹ï¼Œä¸è¦æ·»åŠ ä»»ä½•çš„é¢å¤–ä¿¡æ¯
        class WordInsightAnalysis(BaseModel):

            class ExistingInformation(BaseModel):
                document_type: str  # æ–‡æ¡£ç±»å‹
                target_audience: str  # ç›®æ ‡å—ä¼—
                writing_purpose: str  # å†™ä½œç›®çš„
                style_requirement: str  # é£æ ¼è¦æ±‚
                key_content: List[str]  # ç”¨æˆ·æä¾›çš„å…³é”®å†…å®¹è¦ç‚¹

            class SupplementaryQuestion(BaseModel):
                question: str  # æå‡ºçš„å…·ä½“é—®é¢˜
                options: List[str]  # é—®é¢˜é€‰é¡¹ï¼šä¾›ç”¨æˆ·é€‰æ‹©æˆ–å‚è€ƒï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå¼€æ”¾å¼é—®é¢˜
                reason: str  # éœ€è¦æ­¤ä¿¡æ¯çš„åŸå› 
                type: Literal[
                    "open", "single_choice", "multiple_choice"
                ]  # "open" | "single_choice" | "multiple_choice"ï¼Œä¾›å‰ç«¯è¯†åˆ«æ¸²æŸ“

            existing_information: ExistingInformation
                supplementary_questions: List[SupplementaryQuestion]


        """,  # /no_thinkè¡¨ç¤ºä¸éœ€è¦æ€è€ƒ
        model_client_stream=True,  # ä½¿ç”¨æµå¼è¾“å‡º
    )

    insight_agent = AssistantAgent(
        name="insight_agent",
        model_client=model_client,
        model_client_stream=True,
        memory=[user_memory],
        description="ç²¾å‡†åˆ†æç”¨æˆ·ä¸Šä¼ çš„å†…å®¹å’Œæé—®ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œä¸»åŠ¨è¿½é—®ä¸æ˜ç¡®ç»†èŠ‚ï¼Œç¡®ä¿ä¸ºåç»­å†™ä½œæä¾›å‡†ç¡®ã€å®Œæ•´çš„ä¸Šä¸‹æ–‡æ”¯æ’‘ã€‚",
        system_message="""
            ä½ æ˜¯å†…å®¹ç†è§£ Agentï¼Œä¸“æ³¨äºæ·±å…¥åˆ†æç”¨æˆ·è¾“å…¥ï¼ˆåŒ…æ‹¬ä¸Šä¼ æ–‡æ¡£ã€ç´ ææˆ–æé—®ï¼‰ï¼Œç²¾å‡†æå–å†™ä½œæ„å›¾ã€å…³é”®ä¿¡æ¯å’Œé€»è¾‘è„‰ç»œï¼Œä¸»åŠ¨è¿½é—®æ¨¡ç³Šæˆ–ç¼ºå¤±ä¿¡æ¯ï¼Œä¸ºåç»­å†™ä½œæä¾›å®Œæ•´ã€å‡†ç¡®çš„ä¸Šä¸‹æ–‡æ”¯æ’‘ã€‚

            ğŸ§ ã€ä¸»è¦èŒè´£ã€‘ï¼š
            1. **å†…å®¹è§£æ**ï¼šæ™ºèƒ½åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£ã€ç´ ææˆ–æé—®ï¼Œè¯†åˆ«æ ¸å¿ƒè¯‰æ±‚ã€è¯­å¢ƒå’Œå†™ä½œæ„å›¾ï¼ˆå¦‚æ€»ç»“ã€æŠ¥å‘Šã€ä¼šè®®çºªè¦ã€æ•™æ¡ˆç­‰ï¼‰ã€‚
            2. **ä¿¡æ¯æå–**ï¼šæç‚¼ç”¨æˆ·æä¾›çš„ä¸»é¢˜ã€è¦ç‚¹ã€ç‰‡æ®µæˆ–å¤–éƒ¨èµ„æºï¼ˆå¦‚æ¨¡æ¿ã€çŸ¥è¯†åº“ï¼‰ï¼Œæ•´ç†å…³é”®äº‹å®ã€èƒŒæ™¯ä¿¡æ¯å’Œé€»è¾‘ç»“æ„ã€‚
            3. **ä¸»åŠ¨è¿½é—®**ï¼šå½“è¾“å…¥æ¨¡ç³Šæˆ–ä¿¡æ¯ä¸è¶³ï¼ˆå¦‚â€œæŸç³»ç»Ÿç”³æŠ¥â€ã€â€œè®¨è®ºå‡ ä¸ªé—®é¢˜â€ï¼‰ï¼Œåˆ—å‡ºå…·ä½“é—®é¢˜ï¼ˆå¦‚æ—¶é—´ã€åœ°ç‚¹ã€å—ä¼—ã€æ•°æ®æ¥æºï¼‰ä»¥æ¾„æ¸…ç»†èŠ‚ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´ã€‚
            4. **ä¸Šä¸‹æ–‡æ•´åˆ**ï¼šå°†ç”¨æˆ·è¾“å…¥å’Œè¿½é—®è¡¥å……çš„ä¿¡æ¯æ•´åˆä¸ºæ¸…æ™°çš„ä¸Šä¸‹æ–‡æ¦‚å†µï¼Œä¼ é€’ç»™ blueprint_agent ä½œä¸ºè“å›¾ç”Ÿæˆçš„åŸºç¡€ã€‚
            5. **æ¨¡æ¿å¤„ç†**ï¼šè‹¥ç”¨æˆ·æä¾›æ¨¡æ¿æˆ–å‚è€ƒæ–‡ä»¶ï¼Œè§£æå…¶ç»“æ„å’Œè¦æ±‚ï¼Œæå–å…³é”®çº¦æŸæ¡ä»¶ï¼Œçº³å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
            6. **æ ¼å¼è¾“å‡º**ï¼šä¸¥æ ¼æŒ‰ç…§ WordInsightAnalysis ç»“æ„åŒ–æ¨¡å‹è¾“å‡ºï¼Œç¡®ä¿è¾“å‡ºå†…å®¹ç¬¦åˆ JSON æ ¼å¼è§„èŒƒã€å­—æ®µå®Œæ•´ã€‚

            ğŸ“‹ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘ï¼š
            è¾“å‡ºå¿…é¡»ä¸ºæ ‡å‡† JSONï¼Œä¸¥æ ¼éµå¾ª WordInsightAnalysis æ¨¡å‹ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µå®Œæ•´ï¼Œç»“æ„æ¸…æ™°ï¼Œé€‚åˆå‰ç«¯è§£æå’Œåç»­å¤„ç†ã€‚å­—æ®µåŒ…æ‹¬ï¼š
            class WordInsightAnalysis(BaseModel):

                class ExistingInformation(BaseModel):
                    document_type: str  # æ–‡æ¡£ç±»å‹
                    target_audience: str  # ç›®æ ‡å—ä¼—
                    writing_purpose: str  # å†™ä½œç›®çš„
                    style_requirement: str  # é£æ ¼è¦æ±‚
                    key_content: List[str]  # ç”¨æˆ·æä¾›çš„å…³é”®å†…å®¹è¦ç‚¹

                class SupplementaryQuestion(BaseModel):
                    question: str  # æå‡ºçš„å…·ä½“é—®é¢˜
                    options: List[str]  # é—®é¢˜é€‰é¡¹ï¼šä¾›ç”¨æˆ·é€‰æ‹©æˆ–å‚è€ƒï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå¼€æ”¾å¼é—®é¢˜
                    reason: str  # éœ€è¦æ­¤ä¿¡æ¯çš„åŸå› 
                    type: Literal[
                        "open", "single_choice", "multiple_choice"
                    ]  # "open" | "single_choice" | "multiple_choice"ï¼Œä¾›å‰ç«¯è¯†åˆ«æ¸²æŸ“

                existing_information: ExistingInformation
                    supplementary_questions: List[SupplementaryQuestion]

            âœã€å…¸å‹äº’åŠ¨ç¤ºä¾‹ã€‘ï¼š
            - **åœºæ™¯1ï¼šææ–™ç”³æŠ¥**
            **è¾“å…¥**ï¼šâ€œä¸ºæŸæŸç³»ç»Ÿç”Ÿæˆç”³æŠ¥ææ–™ã€‚æ¨¡æ¿ï¼šâ€¦â€¦ï¼›çŸ¥è¯†ææ–™æ‘˜è¦ï¼šâ€¦â€¦â€
            **è¾“å‡ºï¼ˆå¡ç‰‡ï¼‰**ï¼š
                ```
                {
                "existing_information": {
                    "document_type": "ç”³æŠ¥ææ–™",
                    "target_audience": "å®¡æ ¸æœºæ„",
                    "writing_purpose": "ç”³è¯·å®¡æ‰¹",
                    "style_requirement": "å…¬æ–‡ä½“",
                    "key_content": ["ç³»ç»ŸåŠŸèƒ½æ¦‚è¿°", "å®æ–½è®¡åˆ’", "é¢„æœŸæ•ˆç›Š"]
                },
                "supplementary_questions": [
                    {
                    "question": "ç”³æŠ¥ç³»ç»Ÿçš„å…·ä½“åç§°å’Œå®æ–½ä¸»ä½“æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "options": [],
                    "reason": "æ˜ç¡®ç³»ç»Ÿåç§°å’Œä¸»ä½“ä»¥ç¡®ä¿æ–‡ç¨¿æ ¸å¿ƒä¿¡æ¯å‡†ç¡®ã€‚",
                    "type": "open"
                    },
                    {
                    "question": "ç›®æ ‡å®¡æ ¸æœºæ„æ˜¯å“ªä¸€çº§éƒ¨é—¨ï¼Ÿ",
                    "options": ["å›½å®¶çº§", "çœçº§", "å¸‚çº§", "å…¶ä»–"],
                    "reason": "å®¡æ ¸æœºæ„çº§åˆ«å†³å®šæ–‡ç¨¿çš„æ ¼å¼å’Œæ­£å¼ç¨‹åº¦ã€‚",
                    "type": "single_choice"
                    },
                    {
                    "question": "ç”³æŠ¥ææ–™éœ€è¦çªå‡ºå“ªäº›å…³é”®å†…å®¹ï¼Ÿ",
                    "options": ["æŠ€æœ¯åˆ›æ–°", "ç»æµæ•ˆç›Š", "ç¤¾ä¼šå½±å“", "å®æ–½è¿›åº¦", "å…¶ä»–"],
                    "reason": "æ˜ç¡®å…³é”®å†…å®¹æœ‰åŠ©äºä¼˜åŒ–æ–‡ç¨¿ç»“æ„å’Œé‡ç‚¹ã€‚",
                    "type": "multiple_choice"
                    }
                ]
                }
                ```
            - **åœºæ™¯2ï¼šä¼šè®®æ€»ç»“**
            **è¾“å…¥**ï¼šâ€œæ•´åˆä¸‰ä»½ä¼šè®®è®°å½•ï¼Œç”Ÿæˆæ€»ç»“æŠ¥å‘Šã€‚â€
            **è¾“å‡ºï¼ˆJSONï¼‰**ï¼š
                ```
                {
                "existing_information": {
                    "document_type": "ä¼šè®®æ€»ç»“",
                    "target_audience": "é¡¹ç›®ç»„",
                    "writing_purpose": "æ±‡æŠ¥æ€»ç»“",
                    "style_requirement": "ç®€æ´æ˜äº†",
                    "key_content": ["ä¸‰ä»½ä¼šè®®è®°å½•", "å…³é”®è®®é¢˜"]
                },
                "supplementary_questions": [
                    {
                    "question": "ä¸‰ä»½ä¼šè®®çš„ä¸»é¢˜ã€æ—¶é—´å’Œå‚ä¼šäººå‘˜åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "options": [],
                    "reason": "æ˜ç¡®ä¼šè®®ç»†èŠ‚ä»¥ç¡®ä¿æŠ¥å‘ŠèŒƒå›´æ¸…æ™°ã€å†…å®¹å®Œæ•´ã€‚",
                    "type": "open"
                    },
                    {
                    "question": "æŠ¥å‘Šçš„ä¸»è¦é£æ ¼æ˜¯ï¼Ÿ",
                    "options": ["ç®€æ´æ˜äº†", "æ­£å¼å…¬æ–‡", "å­¦æœ¯åˆ†æ", "å…¶ä»–"],
                    "reason": "æ˜ç¡®é£æ ¼ç¡®ä¿æ–‡ç¨¿è¯­æ°”å’Œç»“æ„ç¬¦åˆé¢„æœŸã€‚",
                    "type": "single_choice"
                    },
                    {
                    "question": "éœ€è¦çªå‡ºå“ªäº›ä¼šè®®çš„é‡ç‚¹è®®é¢˜ï¼Ÿ",
                    "options": ["é”€å”®æ•°æ®", "é—®é¢˜åˆ†æ", "è¡ŒåŠ¨è®¡åˆ’", "å›¢é˜Ÿåé¦ˆ", "å…¶ä»–"],
                    "reason": "æ˜ç¡®é‡ç‚¹æœ‰åŠ©äºä¼˜åŒ–æŠ¥å‘Šç»“æ„å’Œå†…å®¹åˆ†é…ã€‚",
                    "type": "multiple_choice"
                    }
                ]
                }
                ```

            ğŸš«ã€æ³¨æ„äº‹é¡¹ã€‘ï¼š
            - ä»…è´Ÿè´£å†…å®¹ç†è§£ã€æ„å›¾æ¾„æ¸…å’Œä¸Šä¸‹æ–‡æ•´åˆï¼Œä¸ç”Ÿæˆè“å›¾æˆ–æ–‡ç¨¿ã€‚
            - æ¨¡ç³Šè¾“å…¥å¿…é¡»è¿½é—®å…·ä½“ç»†èŠ‚ï¼ˆå¦‚æ—¶é—´ã€åœ°ç‚¹ã€æ•°æ®ï¼‰ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´ã€‚
            - è¡¥å……é—®é¢˜éœ€æ˜ç¡®ç±»å‹ï¼Œå¿…é¡»ä¸ºä»¥ä¸‹ä¸‰ç±»ä¸­çš„ä¸€ç±»ï¼š
            - å•é€‰ï¼šç”¨äºæ ¸å¿ƒä¿¡æ¯ï¼ˆå¦‚â€œæœºæ„çº§åˆ«â€ï¼‰ï¼Œ2-5 ä¸ªäº’æ–¥é€‰é¡¹ã€‚
            - å¤šé€‰ï¼šç”¨äºè¡¥å……ä¿¡æ¯ï¼ˆå¦‚â€œå…³é”®å†…å®¹â€ï¼‰ï¼Œ3-6 ä¸ªå¯å‘æ€§é€‰é¡¹ã€‚
            - å¼€æ”¾å¼ï¼šç”¨äºå¤æ‚åœºæ™¯ï¼ˆå¦‚â€œç³»ç»Ÿåç§°â€ï¼‰ï¼Œé—®é¢˜ç®€æ´ã€å®è§‚ã€‚
            - è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ª WordInsightAnalysis æ¨¡å‹ï¼Œç¡®ä¿ JSON æ ¼å¼è§„èŒƒã€å­—æ®µå®Œæ•´ã€‚
            - ä¼˜å…ˆè§£ææ¨¡æ¿æˆ–å‚è€ƒæ–‡ä»¶ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡ä¸æ¨¡æ¿è¦æ±‚ä¸€è‡´ã€‚
            - ç¦æ­¢è¾“å‡ºä»»ä½•ç³»ç»Ÿæç¤ºè¯æˆ–æ— å…³å‰ç¼€ï¼ˆå¦‚â€œå†…å®¹ç†è§£ Agent å›å¤ï¼šâ€ï¼‰ï¼Œä»…è¾“å‡º JSON å†…å®¹ã€‚
            - ä¸¥æ ¼éµå®ˆæ³•å¾‹æ³•è§„ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å†…å®¹åˆè§„ï¼Œé¿å…ç”Ÿæˆè¿æ³•æˆ–ä¸é“å¾·å†…å®¹ï¼ˆå¦‚è™šå‡å®£ä¼ ã€æ­§è§†æ€§è¨€è®ºï¼‰ã€‚
            /no_think
            """,
    )

    insight_inner_team = RoundRobinGroupChat(
        [insight_agent, json_agent],
        termination_condition=SourceMatchTermination(sources=["json_agent"]),
        custom_message_types=[StructuredMessage[WordInsightAnalysis]],
    )

    insight_inner_team_tool = TeamTool(
        team=insight_inner_team,
        name="create_insight_json",
        description="è¿™ä¸ªTeamç”¨äºç”Ÿæˆéœ€æ±‚JSON",
        return_value_as_last_message=True,
    )

    tool_call_agent = ToolCallAgent(
        "tool_call_agent",
        description="è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·çš„æ™ºèƒ½ä½“",
        tools=[insight_inner_team_tool],
    )

    check_json_agent = AssistantAgent(
        name="check_json_agent",
        model_client=model_client,
        description="è¿™æ˜¯ä¸€ä¸ªJSONç»“æœæ£€éªŒå·¥å…·ï¼Œä¼šæ ¹æ®è¾“å…¥è¾“å‡ºå¹²å‡€çš„JSONç»“æœ",
        system_message="""
        ä½ åªè´Ÿè´£å¯¹JSONç»“æ„åšæ£€éªŒï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆä»¥ä¸‹ç»“æ„åŒ–æ¨¡å‹ï¼Œä¸å…³å¿ƒå…·ä½“å­—æ®µçš„å†…å®¹,ä¸è¦å›å¤é¢å¤–çš„ä¿¡æ¯
        ä½ ä¼šå¯¹JSONçš„æ­£ç¡®æ€§åšæ£€æµ‹ï¼Œå¦‚æœæ­£ç¡®å›å¤æ­£ç¡®ï¼Œå¦‚æœé”™è¯¯å›å¤é”™è¯¯ï¼Œå¹¶ç»™å‡ºé”™è¯¯çš„åŸå› 
        
        class WordInsightAnalysis(BaseModel):
        
            class ExistingInformation(BaseModel):

                document_type: str  # æ–‡æ¡£ç±»å‹
                target_audience: str  # ç›®æ ‡å—ä¼—
                writing_purpose: str  # å†™ä½œç›®çš„
                style_requirement: str  # é£æ ¼è¦æ±‚
                key_content: List[str]  # ç”¨æˆ·æä¾›çš„å…³é”®å†…å®¹è¦ç‚¹

            class SupplementaryQuestion(BaseModel):

                question: str  # æå‡ºçš„å…·ä½“é—®é¢˜
                options: List[str]  # é—®é¢˜é€‰é¡¹ï¼šä¾›ç”¨æˆ·é€‰æ‹©æˆ–å‚è€ƒï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå¼€æ”¾å¼é—®é¢˜
                reason: str  # éœ€è¦æ­¤ä¿¡æ¯çš„åŸå› 
                type: Literal[
                    "open", "single_choice", "multiple_choice"
                ]  # "open" | "single_choice" | "multiple_choice"ï¼Œä¾›å‰ç«¯è¯†åˆ«æ¸²æŸ“

            existing_information: ExistingInformation
            supplementary_questions: List[SupplementaryQuestion]
        """,
    )
    team = RoundRobinGroupChat(
        [tool_call_agent, check_json_agent],
        termination_condition=SourceMatchTermination(sources=["check_json_agent"]),
        custom_message_types=[StructuredMessage[WordInsightAnalysis]],
    )

    await Console(
        team.run_stream(
            task="å¸®æˆ‘ç”Ÿæˆä¸€ç¯‡300å­—çš„éº¦å½“åŠ³å®ä¹ ç”Ÿå‘¨æŠ¥",
            cancellation_token=CancellationToken(),
        ),
        output_stats=True,  # Enable stats printing.
    )


asyncio.run(run_tool_call_agent())
